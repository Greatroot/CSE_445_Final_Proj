{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Ben Kosa's attempt for Google's Individual Sign Language Recognition (ISLR) Kaggle Competition","metadata":{}},{"cell_type":"markdown","source":"## **Let's First Load our Data In and Visualize It**","metadata":{}},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm  # To give us that sweet progress bar during training/testing\nimport json\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.modules.module import Module\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# Sci-kit libraries to help with mundane splitting of data and calculating accuracy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\nimport math\nimport cv2\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-03-18T00:01:56.458676Z","iopub.execute_input":"2023-03-18T00:01:56.459089Z","iopub.status.idle":"2023-03-18T00:01:56.467824Z","shell.execute_reply.started":"2023-03-18T00:01:56.459052Z","shell.execute_reply":"2023-03-18T00:01:56.465403Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/asl-signs/\"\nLANDMARK_DATA_DIR = \"/kaggle/input/asl-signs/train_landmark_files\"\nTRAIN_FILE = \"/kaggle/input/asl-signs/train.csv\"\nlabel_map = json.load(open(\"/kaggle/input/asl-signs/sign_to_prediction_index_map.json\", \"r\"))","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:18:52.091363Z","iopub.execute_input":"2023-03-17T05:18:52.092393Z","iopub.status.idle":"2023-03-17T05:18:52.102726Z","shell.execute_reply.started":"2023-03-17T05:18:52.092341Z","shell.execute_reply":"2023-03-17T05:18:52.101421Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Our data:\n\nThe data we're provided for this challenge is a collection of hand and facial landmarks generated by Mediapipe (v.0.9.0.1) on roughly 100k videos of isolated signs performed by 21 Deaf signers from a vocabulary of 250-signs (meaning 250 classes).\n\nEach training/testing instance can be thought of as (num_frames, 528, 3), where 528 is the total number of landmarks that Mediapipe Holistic maps onto the body. In other words, each data instance is a video of someone performing exactly one sign (or word in ASL). Each video has num_frames number of frames in it and each of the 528 landmarks is a coordinate in 3D space (i.e. x, y, z float values). ","metadata":{}},{"cell_type":"code","source":"# Utilities for Loading Our Data\nROWS_PER_FRAME = 543  # number of landmarks per frame\n\ndef load_relevant_data_subset_with_imputation(pq_path):\n    \"\"\"\n    Loads our data and replaces missing data (NaNs) with zeros.\n    \"\"\"\n    data_columns = ['x', 'y', 'z']\n    data = pd.read_parquet(pq_path, columns=data_columns)\n    data.replace(np.nan, 0, inplace=True)\n    n_frames = int(len(data) / ROWS_PER_FRAME)\n    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n    return data.astype(np.float16)\n\ndef load_relevant_data_subset(pq_path):\n    \"\"\"\n    Just loads our data (without replacing missing data with zeros).\n    \"\"\"\n    data_columns = ['x', 'y', 'z']\n    data = pd.read_parquet(pq_path, columns=data_columns)\n    n_frames = int(len(data) / ROWS_PER_FRAME)\n    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n    return data.astype(np.float32)\n\ndef read_dict(file_path):\n    path = os.path.expanduser(file_path)\n    with open(path, \"r\") as f:\n        dic = json.load(f)\n    return dic","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:19:07.361735Z","iopub.execute_input":"2023-03-17T05:19:07.362644Z","iopub.status.idle":"2023-03-17T05:19:07.372233Z","shell.execute_reply.started":"2023-03-17T05:19:07.362603Z","shell.execute_reply":"2023-03-17T05:19:07.371067Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load our data\n# The csv is only meant to serve as metadata for\n# each actual instance of data, which is stored in a parquet file.\ntrain_df = pd.read_csv(TRAIN_FILE)\ntrain_df.head()  # For visualization.","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:19:08.184852Z","iopub.execute_input":"2023-03-17T05:19:08.185218Z","iopub.status.idle":"2023-03-17T05:19:08.375032Z","shell.execute_reply.started":"2023-03-17T05:19:08.185188Z","shell.execute_reply":"2023-03-17T05:19:08.373710Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                            path  participant_id  sequence_id  \\\n0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n\n    sign  \n0   blow  \n1   wait  \n2  cloud  \n3   bird  \n4   owie  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>participant_id</th>\n      <th>sequence_id</th>\n      <th>sign</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_landmark_files/26734/1000035562.parquet</td>\n      <td>26734</td>\n      <td>1000035562</td>\n      <td>blow</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_landmark_files/28656/1000106739.parquet</td>\n      <td>28656</td>\n      <td>1000106739</td>\n      <td>wait</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_landmark_files/16069/100015657.parquet</td>\n      <td>16069</td>\n      <td>100015657</td>\n      <td>cloud</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_landmark_files/25571/1000210073.parquet</td>\n      <td>25571</td>\n      <td>1000210073</td>\n      <td>bird</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_landmark_files/62590/1000240708.parquet</td>\n      <td>62590</td>\n      <td>1000240708</td>\n      <td>owie</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Understanding the dataset a little better\nprint(f\"Number of unique signers (people) in this dataset: {train_df.participant_id.nunique()}\")\nprint(f\"\\nThe total number of sign videos in this dataset: {len(train_df)}\")\nprint(\"\\nA plot of how many signs each person has recorded:\")\ntrain_df.participant_id.value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:19:08.692261Z","iopub.execute_input":"2023-03-17T05:19:08.692655Z","iopub.status.idle":"2023-03-17T05:19:09.037657Z","shell.execute_reply.started":"2023-03-17T05:19:08.692614Z","shell.execute_reply":"2023-03-17T05:19:09.036648Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Number of unique signers (people) in this dataset: 21\n\nThe total number of sign videos in this dataset: 94477\n\nA plot of how many signs each person has recorded:\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# Making an index_to_label mapping since the output to our model will be in indices\nlabel_to_index = read_dict(f\"../input/asl-signs/sign_to_prediction_index_map.json\")\nindex_to_label = dict([(label_to_index[key], key) for key in label_to_index])\n\n# Add indexed labels to our dataframe\ntrain_df[\"label\"] = train_df[\"sign\"].map(lambda sign: label_to_index[sign])\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:19:09.398932Z","iopub.execute_input":"2023-03-17T05:19:09.399323Z","iopub.status.idle":"2023-03-17T05:19:09.450029Z","shell.execute_reply.started":"2023-03-17T05:19:09.399291Z","shell.execute_reply":"2023-03-17T05:19:09.448921Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                            path  participant_id  sequence_id  \\\n0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n\n    sign  label  \n0   blow     25  \n1   wait    232  \n2  cloud     48  \n3   bird     23  \n4   owie    164  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>participant_id</th>\n      <th>sequence_id</th>\n      <th>sign</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_landmark_files/26734/1000035562.parquet</td>\n      <td>26734</td>\n      <td>1000035562</td>\n      <td>blow</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_landmark_files/28656/1000106739.parquet</td>\n      <td>28656</td>\n      <td>1000106739</td>\n      <td>wait</td>\n      <td>232</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_landmark_files/16069/100015657.parquet</td>\n      <td>16069</td>\n      <td>100015657</td>\n      <td>cloud</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_landmark_files/25571/1000210073.parquet</td>\n      <td>25571</td>\n      <td>1000210073</td>\n      <td>bird</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_landmark_files/62590/1000240708.parquet</td>\n      <td>62590</td>\n      <td>1000240708</td>\n      <td>owie</td>\n      <td>164</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Loading Our Data\n- We have a function for pre-loading all of the data into a numpy array, \n- as well as a function for preloading half of the data at a time and saving them seperately since","metadata":{}},{"cell_type":"code","source":"NUM_VIDEOS = len(train_df)\nNUM_LANDMARKS = 468 + 21 + 33 + 21  # Total = 543\nNUM_LANDMARKS_NO_FACE = 21 + 33 + 21  # Total = 75\nN_SAMPLES = 50","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:19:11.561403Z","iopub.execute_input":"2023-03-17T05:19:11.562387Z","iopub.status.idle":"2023-03-17T05:19:11.568242Z","shell.execute_reply.started":"2023-03-17T05:19:11.562336Z","shell.execute_reply":"2023-03-17T05:19:11.567133Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nGo through every single training instance and create training and testing datasets\nin the form of a tensor in the form (n_videos, n_frames, n_landmarks * 3)\n[[frame_1], [frame_2], ..., [frame_K]] video 1\n[[frame_1], [frame_2], ..., [frame_K]] video 2\n...\n[[frame_1], [frame_2], ..., [frame_K]] video N\n\nwhere each frame contains n_landmarks number of x, y, z points j:\n[j0_x, j0_y, j0_z, j1_x, j1_y, j1_z, ...]\n\nSo in the end, our x_train should be size (n_videos, n_frames, n_landmarks * 3)\n\"\"\"\nx_train = []\ny_train = []\nnum_frames = np.zeros(len(train_df))  # count the number of frames for each instance\nfor i in tqdm(range(len(train_df))):\n    path = f\"{DATA_PATH}{train_df.iloc[i].path}\"\n    data = load_relevant_data_subset_with_imputation(path)\n    # TODO: If having each x, y, z point is too costly, consider creating a lower dim\n    # embedding for each frame\n    x_train.append(data)\n    y_train.append(train_df.iloc[i].label)\n#     num_frames[i] = data.shape[0]\n    \ntrain_df[\"num_frames\"] = num_frames\nx_train = np.array(x_train)\ny_train = np.array(y_train)\ntrain_df.to_csv(\"train.csv\", index=False)  # Save our additions to our metadata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv(\"train.csv\", index=False)\n\nprint(f\"x_train shape: {x_train}\")\nprint(f\"y_train shape: {y_train}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save our numpy arrays as compressed .npy files so we don't have to load them every single time. Be sure to download this locally onto your computer once\nthey're saved to your kaggle/outputs/ folder. This is because after each session, Kaggle will wipe the outputs folder. To save our numpy landmark data\nacross different sessions, download the compressed numpy arrays locally and upload x_train and y_train (both compressed into the file \"numpy_landmark_data.npy.\") as a Dataset to this kaggle notebook.","metadata":{}},{"cell_type":"code","source":"# Save our numpy arrays as compressed .npy files so we don't have to load them every single time\nnp.savez_compressed(\"/kaggle/working/numpy_landmark_data.npy\", x_train=x_train, y_train=y_train)\n# np.save(\"/kaggle/working/x_train.npy\", x_train)\n# np.save(\"/kaggle/working/y_train.npy\", y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Due to limited RAM (13GB), I'll need to only load part of the data by not loading face landmarks.**","metadata":{}},{"cell_type":"code","source":"\"\"\"\nAdd a column to our metadata dataframe that specifies the number of frames\nfor each video\n\"\"\"\nnum_frames = np.zeros(len(train_df))  # count the number of frames for each instance\nfor i in tqdm(range(len(train_df))):\n    path = f\"{DATA_PATH}{train_df.iloc[i].path}\"\n    data = load_relevant_data_subset_with_imputation(path)\n    num_frames[i] = data.shape[0]\n    \ntrain_df[\"num_frames\"] = num_frames","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If you haven't already, save this modified train_df to your output and \n# downloadit to your computer locally so that\ntrain_df.to_csv(\"train_with_n_frames.csv\", index=False)  # Save our additions to our metadata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load in our saved train_df and check to see if the number of frames for each video is the same. If they aren't,\n# this should be ok\ntrain_df = pd.read_csv(\"/kaggle/input/train-df-preloaded/train_with_n_frames.csv\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:19:16.851555Z","iopub.execute_input":"2023-03-17T05:19:16.852043Z","iopub.status.idle":"2023-03-17T05:19:17.101428Z","shell.execute_reply.started":"2023-03-17T05:19:16.852000Z","shell.execute_reply":"2023-03-17T05:19:17.100232Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                            path  participant_id  sequence_id  \\\n0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n\n    sign  label  num_frames  \n0   blow     25        23.0  \n1   wait    232        11.0  \n2  cloud     48       105.0  \n3   bird     23        12.0  \n4   owie    164        18.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>participant_id</th>\n      <th>sequence_id</th>\n      <th>sign</th>\n      <th>label</th>\n      <th>num_frames</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_landmark_files/26734/1000035562.parquet</td>\n      <td>26734</td>\n      <td>1000035562</td>\n      <td>blow</td>\n      <td>25</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_landmark_files/28656/1000106739.parquet</td>\n      <td>28656</td>\n      <td>1000106739</td>\n      <td>wait</td>\n      <td>232</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_landmark_files/16069/100015657.parquet</td>\n      <td>16069</td>\n      <td>100015657</td>\n      <td>cloud</td>\n      <td>48</td>\n      <td>105.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_landmark_files/25571/1000210073.parquet</td>\n      <td>25571</td>\n      <td>1000210073</td>\n      <td>bird</td>\n      <td>23</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_landmark_files/62590/1000240708.parquet</td>\n      <td>62590</td>\n      <td>1000240708</td>\n      <td>owie</td>\n      <td>164</td>\n      <td>18.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# It seems that if I pad the frames for all videos and try to pre-load that,\n# it takes up too much data.\ntrain_df.num_frames.value_counts().sort_values(ascending=False).plot(kind=\"bar\")\nplt.plot()\nplt.xlim(0, 40)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:19:17.122846Z","iopub.execute_input":"2023-03-17T05:19:17.125162Z","iopub.status.idle":"2023-03-17T05:19:19.260155Z","shell.execute_reply.started":"2023-03-17T05:19:17.125120Z","shell.execute_reply":"2023-03-17T05:19:19.258964Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(0.0, 40.0)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\nOnly load in the hand and pose landmarks. Ignore the face landmarks.\nin the form of a tensor in the form (n_videos, n_frames, n_landmarks * 3)\n[[frame_1], [frame_2], ..., [frame_K]] video 1\n[[frame_1], [frame_2], ..., [frame_K]] video 2\n...\n[[frame_1], [frame_2], ..., [frame_K]] video N\n\nwhere each frame contains n_landmarks number of x, y, z points j:\n[j0_x, j0_y, j0_z, j1_x, j1_y, j1_z, ...]\n\nSo in the end, our x_train should be size (n_videos, n_frames, n_landmarks * 3)\n\nALSO keep in mind that we want all videos to have the same number of frames, so\nwe will have to pad some videos with zero-frames to make them all even.\n\"\"\"\n# First find the maximum number of frames out of all videos in our data\n# max_frames = train_df.max(axis='rows')['num_frames']  # the num of frames should be last col\nmax_frames = 20\n\ndata_x = []\ndata_y = []\nfor i in tqdm(range(int(len(train_df)))):\n    \"\"\" Only train on videos that are max_frames frames or less\"\"\"\n    if train_df.iloc[i].num_frames <= max_frames:\n        path = f\"{DATA_PATH}{train_df.iloc[i].path}\"\n        # data: (n_frames, n_landmarks, n_dims) where n_diims = 3\n        data = load_relevant_data_subset_with_imputation(path)\n        # For now, don't include face landmarks:\n    #     face_x = x[:,:468,:].contiguous().view(-1, 468*3) ignore the face landmarks for now due to limited RAM. Keep in mind the face is still important.\n        lefth_x = data[:,468:489,:].reshape(-1, 21*3)\n        pose_x = data[:,489:522,:].reshape(-1, 33*3)\n        righth_x = data[:,522:,:].reshape(-1, 21*3)\n\n    #     print(f\"n_frames for left hand = {lefth_x.shape[0]}\")\n    #     print(f\"n_frames for pose = {pose_x.shape[0]}\")\n    #     print(f\"n_frames for right hand = {righth_x.shape[0]}\")\n    #     print(f\"num frames in train_df: {train_df.iloc[i].num_frames}\")\n\n    #     print(f\"lefth_x shape: {lefth_x.shape}\")\n    #     print(f\"pose_x shape: {pose_x.shape}\")\n    #     print(f\"righth_x shape: {righth_x.shape}\")\n\n        # Convert any NaN landmarks to 0\n        lefth_x = np.nan_to_num(lefth_x)\n        pose_x = np.nan_to_num(pose_x)\n        righth_x = np.nan_to_num(righth_x)\n\n        landmarks = np.concatenate((lefth_x, pose_x, righth_x), axis=1)\n\n        # zero pad our video with extra zero-frames until it has max_frames num\n        # of frames\n        n_frames_to_add = int(max_frames - train_df.iloc[i].num_frames)\n        zero_frame = np.zeros(lefth_x.shape[1] + pose_x.shape[1] + righth_x.shape[1])\n        padding_frames = []\n        for j in range(n_frames_to_add):\n            padding_frames.append(zero_frame)\n\n        if len(padding_frames) != 0:\n            landmarks = np.concatenate((landmarks, np.array(padding_frames)))\n            \n#         landmarks = sequential_sampling(landmarks, train_df.iloc[i].num_frames, N_SAMPLES)zz\n\n        data_x.append(landmarks)\n        data_y.append(train_df.iloc[i].label)\n    \n    #     print(f\"len of data_x: {len(data_x)}\")\n    #     print(f\"i = {i}\")\n    #     print(f\"data_x[i] shape = {data_x[i].shape}\")\n    \ndata_x_no_face = np.array(data_x)\ndata_y_no_face = np.array(data_y)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:19:20.482782Z","iopub.execute_input":"2023-03-17T05:19:20.483225Z","iopub.status.idle":"2023-03-17T05:28:13.672771Z","shell.execute_reply.started":"2023-03-17T05:19:20.483182Z","shell.execute_reply":"2023-03-17T05:28:13.671707Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|██████████| 94477/94477 [08:52<00:00, 177.40it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"data_x_no_face shape = {data_x_no_face.shape}\")\nprint(f\"data_y_no_face shape = {data_y_no_face.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:29:03.043565Z","iopub.execute_input":"2023-03-17T05:29:03.044379Z","iopub.status.idle":"2023-03-17T05:29:03.050060Z","shell.execute_reply.started":"2023-03-17T05:29:03.044339Z","shell.execute_reply":"2023-03-17T05:29:03.048897Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"data_x_no_face shape = (43993, 20, 225)\ndata_y_no_face shape = (43993,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(f\"data_x shape = {data_x_no_face.reshape(len(train_df), -1, NUM_LANDMARKS_NO_FACE * 3)}\")\n# print(f\"data_y shape = {data_y_no_face.reshape(len(train_df), -1, NUM_LANDMARKS_NO_FACE * 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the second half of our numpy arrays as compressed \n# .npy files so we don't have to load them every single time\nnp.savez_compressed(\"/kaggle/working/numpy_landmark_data_no_face_padded\", x_train_no_face=data_x, y_train_no_face=data_y)\n# np.save(\"/kaggle/working/x_train.npy\", x_train)\n# np.save(\"/kaggle/working/y_train.npy\", y_train)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:29:09.230435Z","iopub.execute_input":"2023-03-17T05:29:09.230819Z","iopub.status.idle":"2023-03-17T05:29:59.147182Z","shell.execute_reply.started":"2023-03-17T05:29:09.230784Z","shell.execute_reply":"2023-03-17T05:29:59.146124Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Visualizing what one parquet data file looks like. NOTE, the output of \n# load_relevant_data_subset_with_imputation() is a numpy array of shape \n# (n_frames, 543, 3), where 543 is the number of landmarks and 3 corresponds to x, y, z\n#\npath = f\"{DATA_PATH}{train_df.iloc[0].path}\"\ndata = load_relevant_data_subset_with_imputation(path)\nprint(f\"shape of data: {data.shape}\")\nprint(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretrained Action Classification Models that use Mediapipe Landmarks as input?\n\nI looked, and unfortunately there are no action classification pretrained models that use Mediapipe Landmarks as input, thus pretraining and finetuning isn't an option for this.","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing Utilites\nBefore training our model on our data, it might be useful to convert the landmark data for each video instance (num_frames, 528, 3) to a feature vector (i.e. we are doing some feature engineering in the hopes that this can improve our peformance). While we don't do this currently, it would be something to consider for the future.","metadata":{}},{"cell_type":"code","source":"# Just something I was playing around with.\nclass SimpleFeatureEmbedding(nn.Module):\n    def __init__(self):\n        super(SimpleFeatureEmbedding, self).__init__()\n        pass\n    \n    def forward(self, x):\n        # The first 468 3D coordinate landmarks in each instance are from the face\n        # The next 21 landmarks are from the left hand\n        # The next 33 are from the \"pose\" landmarks (which are just on the arms, wrist,\n        # and shoulders).\n        \n        # We are using .contiguous() to make our tensors continuous in memory \n        # for optimization.\n        face_x = x[:,:468,:].contiguous().view(-1, 468*3)\n        lefth_x = x[:,468:489,:].contiguous().view(-1, 21*3)\n        pose_x = x[:,489:522,:].contiguous().view(-1, 33*3)\n        righth_x = x[:,522:,:].contiguous().view(-1, 21*3)\n        \n        # Prune out any landmarks that are NaN\n        lefth_x = lefth_x[~torch.any(torch.isnan(lefth_x), dim=1),:]\n        righth_x = righth_x[~torch.any(torch.isnan(righth_x), dim=1),:]\n        \n        # Create some really simple feature embedding using the mean and std\n        x1m = torch.mean(face_x, 0)\n        x2m = torch.mean(lefth_x, 0)\n        x3m = torch.mean(pose_x, 0)\n        x4m = torch.mean(righth_x, 0)\n        \n        x1s = torch.std(face_x, 0)\n        x2s = torch.std(lefth_x, 0)\n        x3s = torch.std(pose_x, 0)\n        x4s = torch.std(righth_x, 0)\n        \n        xfeat = torch.cat([x1m,x2m,x3m,x4m, x1s,x2s,x3s,x4s], axis=0)\n        xfeat = torch.where(torch.isnan(xfeat), torch.tensor(0.0, dtype=torch.float32), xfeat)\n        \n        return xfeat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sequential_sampling(video_instance, n_frames, n_samples):\n    \"\"\"\n    Keep sequentially sampling num_samples from the whole video sequence\n    by uniformally skipping frames.\n    \n    :video_instance the landmark data for one video instance\n    :param num_frames the number of frames in this video instance\n    :param num_samples the number of frames we want to sample \n            (the rest will be ignored)\n            \n            \n    :return our pruned ndarray landmark data for this video instance\n            \n    Remember that each video instance is an ndarray of shape \n    (num_frames, num_landmarks * 3) \n    that looks like this: [[frame_1], [frame_2], ..., [frame_K]] video 1\n    where each frame is an array of landmarks in the order\n    [j0_x, j0_y, j0_z, j1_x, j1_y, j1_z, ...]\n    \"\"\"\n    \n    frames_to_sample = []\n    if n_samples < n_frames:\n        # We have too many frames and need to remove some so we only\n        # have num_samples number of frames\n        \n        n_skips = n_frames - n_samples\n        interval = n_frames // n_skips\n        for i, frame_data in enumerate(video_instance):\n            if i % interval == 0 and len(frames_to_sample) <= n_samples:\n                # we aren't skipping this, then add it to our output\n                frames_to_sample.append(frame_data)\n        \n    \n    return frames_to_sample","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:33:39.713129Z","iopub.execute_input":"2023-03-17T05:33:39.713488Z","iopub.status.idle":"2023-03-17T05:33:39.720544Z","shell.execute_reply.started":"2023-03-17T05:33:39.713457Z","shell.execute_reply":"2023-03-17T05:33:39.719493Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def pad_data(video_data):\n    \"\"\"\n    Goes through every video and pads them with extra zero-filled frames\n    so that each video instance has the same number of frames as the video in\n    video_data with the largest number of frames.\n    \"\"\"\n    \n    # First find the maximum number of frames out of all videos in our data\n    max_frames = 0\n    for video in video_data:\n        n_frames = len(video)\n        if n_frames > max_frames:\n            max_frames = n_frames\n            \n    video_data_padded = []\n            \n    for i, video in enumerate(video_data):\n        n_frames = len(video)\n        n_frames_to_pad = max_frames - n_frames\n        \n        frames_list = video.tolist()\n        frame_pad = np.zeros(max_frames)\n        for i in range(n_frames_to_pad):\n            frames_list.append(frame_pad)\n            \n        video_data[i] = np.array(frames_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pad_data(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom Datasets:","metadata":{}},{"cell_type":"code","source":"# Just make a custom Dataset that an load in the data from each instance on the fly?\nclass ASLDataset(Dataset):\n    def __init__(self, x, labels):\n        self.x = x\n        self.labels = labels\n        \n    def __getitem__(self, index):\n        return self.x[index,:], self.labels[index]\n        \n    def __len__(self):\n        return len(self.labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Custom dataset for Pose-TGCN\nclass ASLLandmarkPreloadedDataset(Dataset):\n    \"\"\"\n    Use this if our data is already preloaded into a numpy array.\n    \"\"\"\n    def __init__(self, landmarks, labels):\n        self.landmarks = landmarks\n        self.labels = labels\n        \n        self.n_samples = n_samples\n        \n    def __getitem__(self, vid_idx):\n        instance_data = self.landmarks[vid_idx]\n        n_frames = instance_data.shape[0]\n#         instance_data = sequential_sampling(instance_data, n_frames, self.n_samples)\n        return instance_data, self.labels[vid_idx]\n        \n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T06:34:40.800769Z","iopub.execute_input":"2023-03-17T06:34:40.801918Z","iopub.status.idle":"2023-03-17T06:34:40.809664Z","shell.execute_reply.started":"2023-03-17T06:34:40.801849Z","shell.execute_reply":"2023-03-17T06:34:40.808378Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Custom dataset for Pose-TGCN\nclass ASLLandmarkDataset(Dataset):\n    def __init__(self, video_df, n_samples=50):\n        \"\"\"\n        Use this if our data has NOT been preloaded into a numpy array\n        \n        :params a dataframe with the metadata for every video instance\n        :params \n        \"\"\"\n        self.video_df = video_df\n        self.n_samples = n_samples\n        \n    def __getitem__(self, vid_idx):\n        path = f\"{DATA_PATH}{self.video_df.iloc[i].path}\"\n        # data: (n_frames, n_landmarks, n_dims) where n_diims = 3\n        data = load_relevant_data_subset_with_imputation(path)\n        \n        lefth_x = data[:,468:489,:].reshape(-1, 21*3)\n        pose_x = data[:,489:522,:].reshape(-1, 33*3)\n        righth_x = data[:,522:,:].reshape(-1, 21*3)\n\n        # Convert any NaN landmarks to 0\n        lefth_x = np.nan_to_num(lefth_x)\n        pose_x = np.nan_to_num(pose_x)\n        righth_x = np.nan_to_num(righth_x)\n\n        landmarks = np.concatenate((lefth_x, pose_x, righth_x), axis=1)\n\n        # zero pad our video with extra zero-frames until it has max_frames num\n        # of frames\n        n_frames_to_add = int(max_frames - train_df.iloc[i].num_frames)\n        zero_frame = np.zeros(lefth_x.shape[1] + pose_x.shape[1] + righth_x.shape[1])\n        padding_frames = []\n        for j in range(n_frames_to_add):\n            padding_frames.append(zero_frame)\n\n        if len(padding_frames) != 0:\n            landmarks = np.concatenate((landmarks, np.array(padding_frames)))\n            \n        n_frames = instance_data.shape[0]\n        instance_data = sequential_sampling(instance_data, n_frames, self.n_samples)\n        return self.instance_data, self.sign_labels[vid_idx]\n        \n    def __len__(self):\n        return len(self.labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataloader for SPOTER Network\nclass SpoterDataset(Dataset):\n    \"\"\"\n    A custom Dataset for the data that we input into our SPOTER Network.\n    The data will randomly undergo augmentation before being used.\n    \"\"\"\n    def __init__(self, landmarks, labels, augmentations_prob=0.5):\n        self.landmarks = landmarks\n        self.labels = labels\n        self.augmentations_prob = augmentations_prob\n        \n        \n    def __getitem__(self, vid_idx):\n        instance_data = self.landmarks[vid_idx]\n        n_frames = instance_data.shape[0]\n        if random.random() < self.augmentation_prob:\n            # If augmenting the data is randomly chosen, we then randomly\n            # apply either rotation, squeezing, or perspective shift transforms\n            # to all the landmarks in each frame. \n            rand_aug = random.randrange(3)\n            if rand_aug == 0:\n                augment_rotate()\n                \n                aug_instance_data = augment_rotate(instance)\n        return instance_data, self.labels[vid_idx]\n        \n    def __len__(self):\n        return len(self.labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Our Models:\n\n### Game Plan:\nFor this final project, I am to compare the effectiveness of three different machine learning models on the ASL-Skeleton data provided by this kaggle competition:\n1. a simple Pose-LSTM network (or Pose-GRU if time allows)\n2. Pose-TGCN - https://arxiv.org/abs/1910.11006\n3. SPOTTER (Sign POse-based TransformER) - A slightly modified Transformer (https://paperswithcode.com/paper/sign-pose-based-transformer-for-word-level)","metadata":{}},{"cell_type":"markdown","source":"### 1. LSTM Pose Model","metadata":{}},{"cell_type":"code","source":"class PoseLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, labels_dim, layer_num):\n        super(PoseLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.output_dim = labels_dim\n        \n        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, layer_num)\n#         self.batch_norm = nn.BatchNorm1d(32)\n        self.hidden2label = torch.nn.Linear(hidden_dim, labels_dim)\n        \n    def forward(self, landmarks):\n#         x = self.batch_norm(landmarks)\n        lstm_out, hidden = self.lstm(landmarks)\n        label_space = self.hidden2label(self.hidden_dim, self.labels_dim)\n        label_scores = F.log_softmax(label_space, dim=1)\n        return label_scores","metadata":{"execution":{"iopub.status.busy":"2023-03-17T09:20:32.752514Z","iopub.execute_input":"2023-03-17T09:20:32.752885Z","iopub.status.idle":"2023-03-17T09:20:32.760022Z","shell.execute_reply.started":"2023-03-17T09:20:32.752852Z","shell.execute_reply":"2023-03-17T09:20:32.758979Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"### 2. Pose-Based Temporal Graph Convolution Network (Pose-TGCN)\n\nFrom https://arxiv.org/abs/1910.11006\n\n**NOTE** Pose-TGCN is NOT on torch.hub, unlike ResNet, GoogleNet, etc. So a fair amount of the code is me just copying the model into this kaggle file. If kaggle allowed for multiple files, I would intead just put this library code in a seperate file.","metadata":{}},{"cell_type":"code","source":"class GraphConvolution_att(nn.Module):\n    \"\"\"\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    \"\"\"\n\n    def __init__(self, in_features, out_features, bias=True, init_A=0):\n        super(GraphConvolution_att, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        # A is a K x K adjacency matrix where K = num_vertices (or landmarks)\n        self.A = Parameter(torch.FloatTensor(225, 225))\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        self.A.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input):\n        # AHW\n        support = torch.matmul(input, self.weight)  # HW\n        output = torch.matmul(self.A, support)  # g\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output","metadata":{"execution":{"iopub.status.busy":"2023-03-17T08:48:01.262968Z","iopub.execute_input":"2023-03-17T08:48:01.263432Z","iopub.status.idle":"2023-03-17T08:48:01.288498Z","shell.execute_reply.started":"2023-03-17T08:48:01.263391Z","shell.execute_reply":"2023-03-17T08:48:01.287095Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"class GC_Block(nn.Module):\n\n    def __init__(self, in_features, p_dropout, bias=True, is_resi=True):\n        super(GC_Block, self).__init__()\n        self.in_features = in_features\n        self.out_features = in_features\n        self.is_resi = is_resi\n\n        # TODO: Mess with this to work with MediaPipe Landmarks\n        self.gc1 = GraphConvolution_att(in_features, in_features)\n        self.bn1 = nn.BatchNorm1d(225 * in_features)\n\n        # TODO: Mess with this to work with MediaPipe Landmarks\n        self.gc2 = GraphConvolution_att(in_features, in_features)\n        self.bn2 = nn.BatchNorm1d(225 * in_features)\n        \n        # TODO: Try adding antoher GraphConvolution_att() layer and\n        # see what happens. Try Leaky ReLU instead of Tanh()?\n\n        self.do = nn.Dropout(p_dropout)\n        self.act_f = nn.Tanh()\n\n    def forward(self, x):\n        y = self.gc1(x)\n        b, n, f = y.shape\n        y = self.bn1(y.view(b, -1)).view(b, n, f)\n        y = self.act_f(y)\n        y = self.do(y)\n\n        y = self.gc2(y)\n        b, n, f = y.shape\n        y = self.bn2(y.view(b, -1)).view(b, n, f)\n        y = self.act_f(y)\n        y = self.do(y)\n        if self.is_resi:\n            return y + x\n        else:\n            return y","metadata":{"execution":{"iopub.status.busy":"2023-03-17T08:48:01.761953Z","iopub.execute_input":"2023-03-17T08:48:01.762443Z","iopub.status.idle":"2023-03-17T08:48:01.776867Z","shell.execute_reply.started":"2023-03-17T08:48:01.762397Z","shell.execute_reply":"2023-03-17T08:48:01.775483Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"class GCN_muti_att(nn.Module):\n    def __init__(self, input_feature, hidden_feature, num_class, p_dropout, num_stage=1, is_resi=True):\n        super(GCN_muti_att, self).__init__()\n        self.num_stage = num_stage\n\n        self.gc1 = GraphConvolution_att(input_feature, hidden_feature)\n        self.bn1 = nn.BatchNorm1d(225 * hidden_feature)\n\n        self.gcbs = []\n        for i in range(num_stage):\n            self.gcbs.append(GC_Block(hidden_feature, p_dropout=p_dropout, is_resi=is_resi))\n\n        self.gcbs = nn.ModuleList(self.gcbs)\n\n        # TODO: Try and add this in?\n        # self.gc7 = GraphConvolution_att(hidden_feature, output_feature)\n\n        self.do = nn.Dropout(p_dropout)\n        self.act_f = nn.Tanh()\n\n        # TODO: Try and add this in?\n        # self.fc1 = nn.Linear(75 * output_feature, fc1_out)\n        self.fc_out = nn.Linear(hidden_feature, num_class)\n\n    def forward(self, x):\n        y = self.gc1(x)\n        b, n, f = y.shape\n        y = self.bn1(y.view(b, -1)).view(b, n, f)\n        y = self.act_f(y)\n        y = self.do(y)\n\n        for i in range(self.num_stage):\n            y = self.gcbs[i](y)\n\n        # y = self.gc7(y)\n        out = torch.mean(y, dim=1)\n        out = self.fc_out(out)\n\n        return ","metadata":{"execution":{"iopub.status.busy":"2023-03-17T08:48:11.703101Z","iopub.execute_input":"2023-03-17T08:48:11.703621Z","iopub.status.idle":"2023-03-17T08:48:11.728004Z","shell.execute_reply.started":"2023-03-17T08:48:11.703566Z","shell.execute_reply":"2023-03-17T08:48:11.726977Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"### 3. SPOTER\nFrom https://paperswithcode.com/paper/sign-pose-based-transformer-for-word-level.\n\n**Methodology:**\n- We first concatenate all the landmarks (468 + 21 + 33 + 21 = 543) into a row vector (i.e. [j0_x, j0_y, j0_z, j1_x, j1_y, j1_z, ..., j542_x, j542_y, j542_z]) so that each frame is represented by a 543 * 3 length vector)\n- If no person was located in the frame or any individual landmark could not have been filled, then zeros are filled in their place.\n\n**Augmentations:**\n- To prevent overfitting and boost generalization capability, different spatial augmentations are randomly applied (uniformally) to landmark/skeletal data during training. The spatial augmentation and its parameters are kept consistent for all frames within a sign instance.\n    - *in-plane rotation*\n    - *squeeze*\n    - *perspective transform*\n    - *sequential joint rotation*\n    \n- In my approach, I will only use **in-plane rotation**.\n\n**Normalization:** Incorporates Sign Language linguistic properties; each sign is usually made within the \"Signing Space\" (what literature notes as SS), which is a 3D space between the waist and slightly above the signer's head, spanning transversely from elbow to elbow when both arms are kept loosely bent, and spanning outwards as far as the signer can reach.\n\n***NOTE: MY approach does NOT use Normalization***\n\n***NOTE*** SPOTER is NOT on torch.hub, unlike ResNet, GoogleNet, etc. So a fair amount of the code is me just copying the model into this kaggle file. If kaggle allowed for multiple files, I would intead just put this library code in a seperate file.\n\n***NOTE*** The code below is just code from the SPOTER paper but modified to use Mediapipe input instead of OpenPose input.","metadata":{}},{"cell_type":"code","source":"import copy\nfrom typing import Optional","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Utils for SPOTER**","metadata":{}},{"cell_type":"code","source":"def augment_rotate(frames_data, angle_range):\n    \"\"\"\n    Rotate each frame by a random angle up to 13 degrees with the center of rotation \n    lying in the center of the frame, which is equal to [0.5; 0.5].\n    \n    :param frames_data: ndarray of sequential skeletal data of the signing person\n                 in the form [j0_x, j0_y, j0_z, j1_x, j1_y, j1_z, ...]\n    :param angle_range: tuple containing the angle range \n                        (minimal and maximal angle in degrees) from which we\n                        pick a random angle to rotate by.\n                        \n    :return: ndarray with augmented (by rotation) sequential skeletal data of the signing person\n    \"\"\"\n    angle = math.radians(random.uniform(*angle_range))\n\n    # Rotate each landmark point in the frame counterclockwise by 'angle' degrees\n    # Since we have 3D coordinates, fix the z-axis and only rotate the x and y-axis\n    origin_x, origin_y = (0.5, 0.5)\n    for frame in frames_data:\n        frame_data = frames_data[frame]\n        for i in range(len(frame_data) // 2):\n            px = frame_data[i]\n            py = frame_data[i + 1]\n            frame_data[i] = origin_x + math.cos(angle) * (px - origin_x) - math.sin(angle) * (py - origin_y)\n            frame_data[i + 1] = origin_y + math.sin(angle) * (px - origin_x) - math.cos(angle) * (py - origin_y)\n    \n    return frames_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def augment_shear_squeeze(frames_data, squeeze_ratio):\n#     \"\"\"\n#     Squeeze. All the frames are squeezed from both horizontal sides. Two different random proportions up to 15% of\n#     the original frame's width for both left and right side are cut.\n        \n#     :param frames_data: ndarray of sequential skeletal data of the signing person\n#                  in the form [j0_x, j0_y, j0_z, j1_x, j1_y, j1_z, ...]\n#     :param squeeze_ratio: Tuple containing the relative range from what the \n#                             proportion of the original width will be\n#                             randomly chosen. These proportions will either be cut\n#                             from both sides or used to construct the new projection\n                            \n#     :return: ndarray with squeezed sequential skeletal data of the signing person.\n#     \"\"\"\n#     move_left = random.uniform(*squeeze_ratio)\n#     move_right = random.uniform(*squeeze_ratio)\n\n#     src = np.array(((0, 1), (1, 1), (0, 0), (1, 0)), dtype=np.float32)\n#     dest = np.array(((0 + move_left, 1), (1 - move_right, 1), (0 + move_left, 0), (1 - move_right, 0)), dtype=np.float32)\n#     mtx = cv2.getPerspectiveTransform(src, dest)\n\n#     frames_data\n#     augmented_landmarks = cv2.perspectiveTransform(np.array(frame_data, dtype=np.float32), mtx)\n\n#     augmented_zero_landmark = cv2.perspectiveTransform(np.array([[[0, 0]]], dtype=np.float32), mtx)[0][0]\n#     augmented_landmarks = np.stack([np.where(sub == augmented_zero_landmark, [0, 0], sub) for sub in augmented_landmarks])\n\n#     return augmented_landmarks\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def augment_shear_perspective(frame_data, squeeze_ratio):\n#     \"\"\"\n#     Perspective transformation. The joint coordinates are projected onto a new plane with a spatially defined\n#         center of projection, which simulates recording the sign video with a slight tilt. Each time, the right or left\n#         side, as well as the proportion by which both the width and height will be reduced, are chosen randomly. This\n#         proportion is selected from a uniform distribution on the [0; 1) interval. Subsequently, the new plane is\n#         delineated by reducing the width at the desired side and the respective vertical edge (height) at both of its\n#         adjacent corners.\n        \n#     :param frame_data: ndarray of sequential skeletal data of the signing person\n#                  in the form [j0_x, j0_y, j0_z, j1_x, j1_y, j1_z, ...]\n#     :param squeeze_ratio: Tuple containing the relative range from what the \n#                             proportion of the original width will be\n#                             randomly chosen. These proportions will either be cut\n#                             from both sides or used to construct the new projection\n                            \n#     :return: ndarray with perspective-shifted sequential skeletal data \n#             of the signing person.\n#     \"\"\"\n#     move_ratio = random.uniform(*squeeze_ratio)\n#     src = np.array(((0, 1), (1, 1), (0, 0), (1, 0)), dtype=np.float32)\n\n#     if __random_pass(0.5):\n#         dest = np.array(((0 + move_ratio, 1 - move_ratio), (1, 1), (0 + move_ratio, 0 + move_ratio), (1, 0)), dtype=np.float32)\n#     else:\n#         dest = np.array(((0, 1), (1 - move_ratio, 1 - move_ratio), (0, 0), (1 - move_ratio, 0 + move_ratio)), dtype=np.float32)\n\n#     mtx = cv2.getPerspectiveTransform(src, dest)\n\n#     augmented_landmarks = cv2.perspectiveTransform(np.array(frame_data, dtype=np.float32), mtx)\n\n#     augmented_zero_landmark = cv2.perspectiveTransform(np.array([[[0, 0]]], dtype=np.float32), mtx)[0][0]\n#     augmented_landmarks = np.stack([np.where(sub == augmented_zero_landmark, [0, 0], sub) for sub in augmented_landmarks])\n\n#     return augmented_landmarks","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _get_clones(mod, n):\n    return nn.ModuleList([copy.deepcopy(mod) for _ in range(n)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Model**","metadata":{}},{"cell_type":"code","source":"class SPOTERTransformerDecoderLayer(nn.TransformerDecoderLayer):\n    \"\"\"\n    Edited TransformerDecoderLayer implementation omitting the redundant self-attention operation as opposed to the\n    standard implementation.\n    \"\"\"\n\n    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n        super(SPOTERTransformerDecoderLayer, self).__init__(d_model, nhead, dim_feedforward, dropout, activation)\n\n        del self.self_attn\n\n    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None,\n                memory_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None,\n                memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n\n        tgt = tgt + self.dropout1(tgt)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n                                   key_padding_mask=memory_key_padding_mask)[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n\n        return tgt\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SPOTER(nn.Module):\n    \"\"\"\n    Implementation of the SPOTER (Sign POse-based TransformER) architecture for sign language recognition from sequence\n    of skeletal data.\n    \"\"\"\n\n    def __init__(self, num_classes, hidden_dim=55):\n        super().__init__()\n\n        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim))\n        self.pos = nn.Parameter(torch.cat([self.row_embed[0].unsqueeze(0).repeat(1, 1, 1)], dim=-1).flatten(0, 1).unsqueeze(0))\n        self.class_query = nn.Parameter(torch.rand(1, hidden_dim))  # This is done to get rid of the query class\n        self.transformer = nn.Transformer(hidden_dim, 9, 6, 6)\n        self.linear_class = nn.Linear(hidden_dim, num_classes)\n\n        # Deactivate the initial attention decoder mechanism\n        custom_decoder_layer = SPOTERTransformerDecoderLayer(self.transformer.d_model, self.transformer.nhead, 2048,\n                                                             0.1, \"relu\")\n        self.transformer.decoder.layers = _get_clones(custom_decoder_layer, self.transformer.decoder.num_layers)\n\n    def forward(self, inputs):\n        h = torch.unsqueeze(inputs.flatten(start_dim=1), 1).float()\n        h = self.transformer(self.pos + h, self.class_query.unsqueeze(0)).transpose(0, 1)\n        res = self.linear_class(h)\n\n        return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Augmentation Functions\nFrom https://paperswithcode.com/paper/sign-pose-based-transformer-for-word-level","metadata":{}},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:34:04.073526Z","iopub.execute_input":"2023-03-17T05:34:04.073897Z","iopub.status.idle":"2023-03-17T05:34:04.078942Z","shell.execute_reply.started":"2023-03-17T05:34:04.073864Z","shell.execute_reply":"2023-03-17T05:34:04.077788Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Load our data from our pre_loaded x_train and y_train landmark ndarrays\ndata_x = np.load(\"/kaggle/working/numpy_landmark_data_80p.npz\", allow_pickle=True)\ndata_y = np.load(\"/kaggle/working/numpy_landmark_data_80p.npz\", allow_pickle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_x = data_x_no_face\ndata_y = data_y_no_face","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:34:11.262644Z","iopub.execute_input":"2023-03-17T05:34:11.263048Z","iopub.status.idle":"2023-03-17T05:34:11.284558Z","shell.execute_reply.started":"2023-03-17T05:34:11.263012Z","shell.execute_reply":"2023-03-17T05:34:11.283440Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Create our training, val, and test sets\ntrain_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.15, random_state=42)\ntrain_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.15, random_state=42)\n\ntrain_x = torch.from_numpy(train_x)\ntrain_y = torch.from_numpy(train_y)\n\nval_x = torch.from_numpy(val_x)\nval_y = torch.from_numpy(val_y)\n\ntest_x = torch.from_numpy(test_x)\ntest_y = torch.from_numpy(test_y)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-17T05:34:14.548557Z","iopub.execute_input":"2023-03-17T05:34:14.548940Z","iopub.status.idle":"2023-03-17T05:34:16.707812Z","shell.execute_reply.started":"2023-03-17T05:34:14.548905Z","shell.execute_reply":"2023-03-17T05:34:16.706726Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Print dims to make sure data was loaded in correctly\nprint(f\"dims of train_x: {train_x.shape}\")\nprint(f\"dims of train_y: {train_y.shape}\")\nprint(f\"dims of val_x: {val_x.shape}\")\nprint(f\"dims of val_y: {val_y.shape}\")\nprint(f\"dims of test_x: {test_x.shape}\")\nprint(f\"dims of test_y: {test_y.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:34:18.835172Z","iopub.execute_input":"2023-03-17T05:34:18.835977Z","iopub.status.idle":"2023-03-17T05:34:18.844215Z","shell.execute_reply.started":"2023-03-17T05:34:18.835936Z","shell.execute_reply":"2023-03-17T05:34:18.842678Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"dims of train_x: torch.Size([31784, 20, 225])\ndims of train_y: torch.Size([31784])\ndims of val_x: torch.Size([5610, 20, 225])\ndims of val_y: torch.Size([5610])\ndims of test_x: torch.Size([6599, 20, 225])\ndims of test_y: torch.Size([6599])\n","output_type":"stream"}]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using {device} device\")","metadata":{"execution":{"iopub.status.busy":"2023-03-17T05:34:23.352409Z","iopub.execute_input":"2023-03-17T05:34:23.352819Z","iopub.status.idle":"2023-03-17T05:34:23.473342Z","shell.execute_reply.started":"2023-03-17T05:34:23.352782Z","shell.execute_reply":"2023-03-17T05:34:23.472113Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **Training for Pose-LSTM**","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    Training for PoseLSTM\n\"\"\"\nEPOCHS = 10\nBATCH_SIZE = 64\nFRAME_DIM = (21 + 33 + 21) * 225  # Total = 225\nHIDDEN_DIM = 32  # Decrease if too much\nLABELS_DIM = len(train_y)\nNUM_LAYERS = 1\n\nDROPOUT = 0.3\nLOG_INTERVAL = 1\n\nINIT_LR = 0.001\nEPS = 1e-3  # term added to the denominator for Adam optim. to improve numerical stability (default: 1e-8)\nDECAY = 0\n\ntrain_data = ASLLandmarkPreloadedDataset(train_x, train_y)\nval_data = ASLLandmarkPreloadedDataset(val_x, val_y)\n\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n#  input_dim, hidden_dim, labels_dim, layer_num\nmodel = PoseLSTM(FRAME_DIM, HIDDEN_DIM, LABELS_DIM, NUM_LAYERS).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=INIT_LR, eps=EPS, weight_decay=DECAY)\ncriterion = nn.CrossEntropyLoss()\nsched = torch.optim.lr_scheduler.StepLR(opt, step_size=300, gamma=0.95)\n\nfor i in range(EPOCHS):\n    model.train()\n    \n    train_loss_sum = 0.\n    train_correct = 0\n    train_total = 0\n    train_bar = train_loader\n    for x,y in train_bar:\n        x = torch.Tensor(x).float().cuda()\n        y = torch.Tensor(y).long().cuda()  \n        y_pred = model(x)\n        \n        loss = criterion(y_pred, y)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n        \n        train_loss_sum += loss.item()\n        train_correct += np.sum((np.argmax(y_pred.detach().cpu().numpy(), axis=1) == y.cpu().numpy()))\n        train_total += 1\n        sched.step()\n        \n    val_loss_sum = 0.\n    val_correct = 0\n    val_total = 0\n    model.eval()\n    for x,y in val_loader:\n        x = torch.Tensor(x).float().cuda()\n        y = torch.Tensor(y).long().cuda()\n        \n        with torch.no_grad():\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n            val_loss_sum += loss.item()\n            val_correct += np.sum((np.argmax(y_pred.cpu().numpy(), axis=1) == y.cpu().numpy()))\n            val_total += 1\n                              \n    print(f\"Epoch:{i} > Train Loss: {(train_loss_sum/train_total):.04f}, Train Acc: {train_correct/len(train_data):0.04f}\")\n    print(f\"Epoch:{i} > Val Loss: {(val_loss_sum/val_total):.04f}, Val Acc: {val_correct/len(valid_data):0.04f}\")\n    print(\"=\"*50)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T09:20:37.973145Z","iopub.execute_input":"2023-03-17T09:20:37.973612Z","iopub.status.idle":"2023-03-17T09:20:38.112078Z","shell.execute_reply.started":"2023-03-17T09:20:37.973569Z","shell.execute_reply":"2023-03-17T09:20:38.109156Z"},"trusted":true},"execution_count":72,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_80/3837966924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_80/3992583798.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, landmarks)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         x = self.batch_norm(landmarks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlandmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlabel_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlabel_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    695\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                            ):\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m    699\u001b[0m                                'Expected hidden[0] size {}, got {}')\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    210\u001b[0m             raise RuntimeError(\n\u001b[1;32m    211\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 212\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 16875, got 225"],"ename":"RuntimeError","evalue":"input.size(-1) must be equal to input_size. Expected 16875, got 225","output_type":"error"}]},{"cell_type":"markdown","source":"#### **Training for Pose-TGCN**","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    Pose-TGCN\n# \"\"\"\nEPOCHS = 10\nBATCH_SIZE = 64\nHIDDEN_DIM = 64  # Decrease if too much\nNUM_STAGES = 20\nNUM_CLASSES = len(train_y)\n\nNUM_SAMPLES = 20  # only skip frames during training. TODO: _ is the smallest num of frames in all of our videos\nDROPOUT = 0.3\nLOG_INTERVAL = 1\n\nINIT_LR = 0.001\nEPS = 1e-3  # term added to the denominator for Adam optim. to improve numerical stability (default: 1e-8)\nDECAY = 0\n\ntrain_data = ASLLandmarkPreloadedDataset(train_x, train_y)\nval_data = ASLLandmarkPreloadedDataset(val_x, val_y)\n\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\nmodel = GCN_muti_att(input_feature=NUM_SAMPLES*3, hidden_feature=NUM_SAMPLES*3,\n                     num_class=NUM_CLASSES, p_dropout=DROPOUT, num_stage=NUM_STAGES).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=INIT_LR, eps=EPS, weight_decay=DECAY)\ncriterion = nn.CrossEntropyLoss()\n# sched = torch.optim.lr_scheduler.StepLR(opt, step_size=300, gamma=0.95)\n\nepoch_training_losses = []  # losses for training set for every epoch\nepoch_train_scores = []  # accruacy scores for training set for every epoch\nepoch_val_losses = []  # losses for val set for every epoch\nepoch_val_scores = []  # accuracy scores for val set for every epoch\n\nbest_acc = 0\nfor i in range(EPOCHS):\n    model.train()\n    \n    losses = []\n    scores = []\n    train_labels = []\n    train_preds = []\n    \n    n_samples = 0. # use this to count the total trained samples in one epoch\n    for batch_idx, data in enumerate(train_loader):\n        landmarks, labels = data\n        landmarks = torch.Tensor(landmarks).float().to(device)\n        labels = torch.Tensor(labels).long().cuda().to(device)\n        \n        n_samples += landmarks.size()[0]\n        opt.zero_grad() \n        \n        print(f\"landmarks shape: {landmarks.shape}\")\n        out = model(landmarks)  # out has dim = (batch, num of classes)\n        \n        loss = criterion(y_pred, y)\n        loss.backward()\n        opt.step()\n        \n        # Compute the accuracy for this batch and add it to our list\n        y_pred = torch.max(out, 1)[1]\n        truths = labels.cpu().data.squeeze()\n        preds = y_pred.cpu().data.squeeze()\n        acc = accuracy_score(truths.numpy(), preds.numpy())\n        scores.append()\n        \n        train_labels.extend(truths.tolist())\n        train_preds.extend(y_pred.tolist())\n#         sched.step()\n\n        # show information\n        if (batch_idx + 1) % LOG_INTERVAL == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.6f}%'.format(\n                epoch + 1, n_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(),\n                100 * step_score))\n            \n    \"\"\"Run validation\"\"\"\n    val_loss_sum = 0.\n    val_correct = 0\n    val_total = 0\n    model.eval()\n    for x,y in val_loader:\n        x = torch.Tensor(x).float().cuda()\n        y = torch.Tensor(y).long().cuda()\n        \n        with torch.no_grad():\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n            val_loss_sum += loss.item()\n            val_correct += np.sum((np.argmax(y_pred.cpu().numpy(), axis=1) == y.cpu().numpy()))\n            val_total += 1\n                              \n    print(f\"Epoch:{i} > Train Loss: {(train_loss_sum/train_total):.04f}, Train Acc: {train_correct/len(train_data):0.04f}\")\n    print(f\"Epoch:{i} > Val Loss: {(val_loss_sum/val_total):.04f}, Val Acc: {val_correct/len(valid_data):0.04f}\")\n    print(\"=\"*50)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T09:03:18.911454Z","iopub.execute_input":"2023-03-17T09:03:18.911838Z","iopub.status.idle":"2023-03-17T09:03:19.052903Z","shell.execute_reply.started":"2023-03-17T09:03:18.911805Z","shell.execute_reply":"2023-03-17T09:03:19.051263Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"landmarks shape: torch.Size([64, 20, 225])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_80/400600727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"landmarks shape: {landmarks.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlandmarks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# out has dim = (batch, num of classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_80/3965474568.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_80/663167799.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# AHW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# HW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# g\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1280x225 and 60x60)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1280x225 and 60x60)","output_type":"error"}]},{"cell_type":"code","source":"\"\"\"\n    Pose-TGCN\n# \"\"\"\nEPOCHS = 10\nBATCH_SIZE = 64\nHIDDEN_DIM = 64  # Decrease if too much\nNUM_STAGES = 20\nNUM_CLASSES = len(train_y)\n\nNUM_SAMPLES = 20  # only skip frames during training. TODO: _ is the smallest num of frames in all of our videos\nDROPOUT = 0.3\nLOG_INTERVAL = 1\n\nINIT_LR = 0.001\nEPS = 1e-3  # term added to the denominator for Adam optim. to improve numerical stability (default: 1e-8)\nDECAY = 0\n\ntrain_data = ASLLandmarkPreloadedDataset(train_x, train_y)\nval_data = ASLLandmarkPreloadedDataset(val_x, val_y)\n\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\nmodel = SPOTER(num_classes=NUM_CLASSES, hidden_dim=HIDDEN_DIM).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=INIT_LR, eps=EPS, weight_decay=DECAY)\ncriterion = nn.CrossEntropyLoss()\n# sched = torch.optim.lr_scheduler.StepLR(opt, step_size=300, gamma=0.95)\n\nepoch_training_losses = []  # losses for training set for every epoch\nepoch_train_scores = []  # accruacy scores for training set for every epoch\nepoch_val_losses = []  # losses for val set for every epoch\nepoch_val_scores = []  # accuracy scores for val set for every epoch\n\nbest_acc = 0\nfor i in range(EPOCHS):\n    model.train()\n    \n    losses = []\n    scores = []\n    train_labels = []\n    train_preds = []\n    \n    n_samples = 0. # use this to count the total trained samples in one epoch\n    for batch_idx, data in enumerate(train_loader):\n        landmarks, labels = data\n        landmarks = torch.Tensor(landmarks).float().to(device)\n        labels = torch.Tensor(labels).long().cuda().to(device)\n        \n        n_samples += landmarks.size()[0]\n        opt.zero_grad() \n        \n        print(f\"landmarks shape: {landmarks.shape}\")\n        out = model(landmarks)  # out has dim = (batch, num of classes)\n        \n        loss = criterion(y_pred, y)\n        loss.backward()\n        opt.step()\n        \n        # Compute the accuracy for this batch and add it to our list\n        y_pred = torch.max(out, 1)[1]\n        truths = labels.cpu().data.squeeze()\n        preds = y_pred.cpu().data.squeeze()\n        acc = accuracy_score(truths.numpy(), preds.numpy())\n        scores.append()\n        \n        train_labels.extend(truths.tolist())\n        train_preds.extend(y_pred.tolist())\n#         sched.step()\n\n        # show information\n        if (batch_idx + 1) % LOG_INTERVAL == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.6f}%'.format(\n                epoch + 1, n_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(),\n                100 * step_score))\n            \n    \"\"\"Run validation\"\"\"\n    val_loss_sum = 0.\n    val_correct = 0\n    val_total = 0\n    model.eval()\n    for x,y in val_loader:\n        x = torch.Tensor(x).float().cuda()\n        y = torch.Tensor(y).long().cuda()\n        \n        with torch.no_grad():\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n            val_loss_sum += loss.item()\n            val_correct += np.sum((np.argmax(y_pred.cpu().numpy(), axis=1) == y.cpu().numpy()))\n            val_total += 1\n                              \n    print(f\"Epoch:{i} > Train Loss: {(train_loss_sum/train_total):.04f}, Train Acc: {train_correct/len(train_data):0.04f}\")\n    print(f\"Epoch:{i} > Val Loss: {(val_loss_sum/val_total):.04f}, Val Acc: {val_correct/len(valid_data):0.04f}\")\n    print(\"=\"*50)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Future Work:\n- Try pretraining all three models on Word-Level American Sign Language Dataset. The dataset has videos in RGB, so each video sample would have to be run through Mediapipe to extract all 543 landmarks from each frame.\n- Try using the normalization strategy used by SPOTER.\n- Convert landmarks to heatmaps and train PoseC3D (a 3D CNN approach to skeleton-based action recognition, which is a space where Graph Convolutional Networks (GCNs) are commonly used.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}