# CSE_445_Final_Proj
#### Ben Kosa's Final Project for CSE 455 Wi23.

#### **Website Link:** [ISLR Kaggle Competition - Ben Kosa's Submission](https://bkosa2.wixsite.com/islr-kaggle-competit)

## Problem Description:

Sign Language Recognition, the sign language equivalent of speech recognition, is an extremely important technology as it is the fundamental technology underlying sign-to-text (e.g. translating from a sign in American Sign Language to English text) and text-to-sign capability. Sign Language Recognition comes in two flavors, Word-Level Sign Language recognition and Sentence-Level Sign Language Recognition, and the most modern action recognition algorithms tested for the first task (e.g. Rodriguez et al., 2020; Boháček et al., 2022)  have not been accurate enough yet to start applying in the real world. Sentence-Level Sign Language Recognition with Deep Learning requires first achieving some level of success in Word-Level Sign Language Recognition, so most of the previous work that has been done so far on Sign Language Recognition focuses on recognizing individual signs (i.e. individual words).

There are many exciting applications for Sign Language Recognition, but where it is most needed is removing the communication barriers that Deaf people within the U.S., and across the world constantly face. Sign-Language Recognition can not only remove these barriers by providing a way for hearing and Deaf people to communicate (i.e. translation), but by also providing a way for people to learn sign language so that there is no need for translation. Everyday within the United States, 33 babies are born with permanent hearing loss. Around 90% of these children are born to hearing parents, many of which may not know sign language. Due to sign language not being used within the home and a lack of access to naturally occurring language acquisition during their critical language-learning years, many deaf children are at risk of Language Deprivation. Not acquiring an L1 language (or first language) during the critical period of your life can cause serious impacts on different aspects of your life, such as relationships, education, and employment.

A good start to addressing this disparity in L1 language acquisition is to help hearing parents of d/Deaf children learn basic sign language at the very least. This is an important step in ensuring that d/Deaf children are exposed to sign language early. A team of developers colaborating between Georgia Institutue of Technology (Georgia Tech), the National Technical Institute for the Deaf at Rochester Institute of Technology (NTID or RIT), and Google are developing a smartphone game called PopSign, which aims to make learning basic American Sign Language (ASL) fun, interactive, and accessible (as there are many hearing parents with d/Deaf children who often don’t have the time or resources to learn an entirely new language). To make the game work, it requires an efficient Word-Level Sign Language Recognition model, which Google hopes to find though their Isolated Sign Language Recognition (ISLR) Kaggle Competition. 

## Datasets:

The data we're provided for this challenge is a collection of hand and facial landmarks generated by MediaPipe Holisitc (v.0.9.0.1) on roughly 100k videos (94,477 to be exact) of isolated signs performed by 21 Deaf signers from a vocabulary of 250-signs (meaning 250 classes). MediaPipe Holistic is Google’s open-source pipeline for taking in RGB images and performing Pose Estimation to map skeleton landmarks across any human bodies in an image. MediaPipe Holisitc tries to include enough landmarks to estimate all the important joints across the body, meaning it includes points for the hands (left and right), the torso and arms (what MediaPipe calls Pose Landmarks), and the face. The nice thing about MediaPipe Holistic is that it has been optimized to work well in real-time on low-resource devices (i.e. mobile devices).

Each training/testing instance can be thought of as (num_frames, 543, 3), where 543 is the total number of landmarks that MediaPipe Holistic maps onto the body. In other words, each data instance is a video of someone performing exactly one sign (or word in ASL). Each video has num_frames number of frames in it and each of the 543 landmarks is a coordinate in 3D space (i.e. x, y, z float values), hence why each frame in a each video have 543 * 3 (1,629) landmarks.


## Previous Work:

Word-Level Sign Language Recognition algorithms are a subset of action recognition, and like action recognition, is currently dominated by deep learning approaches. Since around 2018, action recognition approaches come in three different flavors: an RGB-input approach, a skeleton-landmarks-input approach (commonly using algorithms built on top of Graph Convolutional Networks (GCNs)), and a skeleton-heatmap-input approach. The last two are considered to be “skeleton-based” action recognition approaches. For the first approach, it is common to use algorithms built on top of 3D CNN architectures, and although this is the most computationally expensive method, this approach has the highest accuracy today. The third approach also commonly uses 3D CNN based algorithms, but with skeleton heatmaps as input which greatly reduces the amount of input and computation needed. For the second approach, the most commonly used algorithms are built on top of Graph Convolutional Networks (GCNs), which can be extended to model the spatiotemporal relationships between our different feature points using a fully connected graph.

For the MediaPipe landmark data that Google’s ISLR Competition, I use several skeleton-based action recognition algorithms, all three of which have already been tested specifically on a common baseline Word-Level Sign Language dataset called Word-Level American Sign Language (WLASL). WLASL was released in 2020 and consists of 21,083 videos of different people performing exactly one sign or word (Li., et al, 2020). The dataset consisted of 2,000 signs (or classes). The first skeleton-based action recognition algorithm I used was for baseline, for which a LSTM (or Pose-LSTM) is commonly used (e.g. Li et al, 2020; Boháček et al., 2022). The second skeleton-based action recognition algorithm used was a pose-based temporal graph convolutional nerual network, which Li et al. evaluated on the WLASL dataset (Pose-TGCN; Li et al., 2020). The third skeleton-based action recognition algorithm is a transformer based method called Sign POse-based TransformER (SPOTER; Boháček et al., 2022).

## My approach:

Those previous works have been tested on a large set Word-Level Sign Language dataset with 2000 classes where a skeleton-mapping has been extracted from each video using OpenPose (the results of which are around 60 to 70% accuracy), but they all have yet to be tested on Google and RIT’s ISLR Competition dataset, which includes baby signs. These methods have also yet to be tested on landmarks that have been extracted from MediaPipe, which has significantly more landmark points (543) compared to OpenPose (55 landmarks). For this project, I attempted to test the ISLR dataset on Pose-LSTM, Pose-TGCN, and SPOTER in Kaggle Notebook.

Due to the incredibly large size of the dataset, I spent a lot of time trying to pre-load a scaled-down version of the data into a numpy array. This was due to the fact that Kaggle Notebook only allowed for 13 GigaBytes of ram, and the entire dataset with padding was around 20 to 30 GB. As the face isn’t often used in baby signs, I removed the face-landmarks from the data and reduced the number of landmarks per frame from 468 + 21 + 33 + 21 = 543 to 21 + 33 + 21 = 75 landmarks. The number of frames in each video varied, so preprocessing was needed to pad each video so that they all had the same number of frames (which would allow for batch training). 

The number of frames ranged from 6 to 537, but the majority of videos ranged between 6 to 50 frames. To reduce the amount of data even more so I could preload it, I only allowed videos with 20 frames or less. In the end, my preprocessed dataset had around 43,993 training instances compared to the original 94,477. I would have used a pre-trained version of Pose-TGCN and SPOTER, but the only pre-trained models available were ones that were pre trained on WLASL with landmarks extracted using OpenPose, which uses different landmarks than MediaPipe. This was all the pre-processing done for Pose-LSTM and Pose-TGCN.
  
A little bit more pre-processing was done for the SPOTER model. In Boháček et al., SPOTER incorporates Sign Language linguistic properties using a unique normalization process. Each sign is usually made within the "Signing Space" (what literature notes as SS), which is a 3D space between the waist and slightly above the signer's head, spanning transversely from elbow to elbow when both arms are kept loosely bent, and spanning outwards as far as the signer can reach. Bounding boxes are found for the signing space (SS), the left hand, and the right hand and all of the landmarks detected are filtered out depending on whether they are actually in the signing space. Each landmark is also normalized to be relative to their respective bounding boxes (e.g. left-hand landmarks are now (x, y, z) with respect to the coordinate system of the left-hand bounding box). In our code, we do not normalize the data.
  
However, we do follow Boháček et al. in augmenting the frames in each video by randomly choosing one of three different transforms to apply to every frame: (1) in-plane rotation, (2) squeeze, and (3) perspective transform. This helps prevent overfitting and boosts generalization capability since we are diversifying our data through transformations/noise.

## Results:

Unfortunately, I was unable to get any of the three models to work in time. There are no conclusive results to show.

## Discussion:

Even though I was unable to successfully train and test all three models and make a submission for this Kaggle competition (the due date is in April, so I do plan on finishing in my own time to make a submission), I learned A LOT about some of the modern approaches to action classification, and more excitingly sign language classification. Future work includes getting the current slimmed-down version of the video landmark data to work with my baseline Pose-LSTM, Pose-TGCN, and SPOTER. From there, I would like to move to a system that has more ram and more GPU resources, as this would allow me to train on the full video landmark ISLR Kaggle Competition dataset. This would also allow me to pre-train Pose_TGCN and SPOTER on the WLASL dataset after having fed it through MediaPipe, which I have already personally verified takes more than one GPU to train on due to the large amount of frames/images in total. I could also make this WLASL-Skeletion dataset publicly available along with the pretrained weights from the two models for others to use.

For the SPOTER network, future work could also include trying to use the normalization strategy used by Boháček et al. to incorporate sign language linguistic heuristics to improve accuracy. I would also like to try using the third flavor of deep action recognition that’s starting to gain popularity: converting the landmark data to heatmaps and training a Pose3CD model (Duan et al., 2022).


Citation:

Li, D., Rodriguez, C., Yu, X., & Li, H. (2020). Word-level deep sign language recognition from video: A 
     new large-scale dataset and methods comparison. In Proceedings of the IEEE/CVF winter conference on 
     applications of computer vision (pp. 1459-1469).

Boháček, M., & Hrúz, M. (2022). Sign pose-based transformer for word-level sign language recognition. In 
     Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 182-191).

Duan, H., Zhao, Y., Chen, K., Lin, D., & Dai, B. (2022). Revisiting skeleton-based action recognition. In 
     Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2969-2978).

